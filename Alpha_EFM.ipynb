{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967522b2-c5eb-4b6b-bb20-f3a28559e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from typing import Union\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f844ea-29bc-47d2-9b7c-561812cb207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax_dataclasses as jdc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c4122-acbd-4d0d-8e36-e37e4cc5343b",
   "metadata": {},
   "source": [
    "# 1️⃣ Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e86c866f-55c5-41ca-8c27-b5f2aff3afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Longueur d’un mot (nombre de lettres)\n",
    "# -------------------------------------------------\n",
    "@jax.jit\n",
    "def word_len(word: Union[int, jax.Array]) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Retourne la longueur d’un mot représenté comme un entier.\n",
    "    Exemples :\n",
    "        0   -> 0\n",
    "        1   -> 1\n",
    "        12  -> 2\n",
    "        231 -> 3\n",
    "    \"\"\"\n",
    "    return jnp.where(\n",
    "        word == 0,\n",
    "        0,\n",
    "        jnp.floor(jnp.log10(word) + 1e-10) + 1\n",
    "    ).astype(int)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Nombre total de mots de longueur <= trunc\n",
    "# -------------------------------------------------\n",
    "@jax.jit\n",
    "def number_of_words_up_to_trunc(trunc: Union[int, jax.Array], dim: int) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Calcule le nombre total de mots sur un alphabet de taille `dim`\n",
    "    dont la longueur est <= trunc.\n",
    "\n",
    "    Formellement :\n",
    "        1 + dim + dim^2 + ... + dim^trunc\n",
    "    \"\"\"\n",
    "    return jnp.maximum(\n",
    "        (dim ** (trunc + 1) - 1) // (dim - 1),\n",
    "        trunc + 1\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Longueur du mot associé à un index\n",
    "# -------------------------------------------------\n",
    "@jax.jit\n",
    "def index_to_word_len(index: Union[int, jax.Array], dim: int) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Donne la longueur du mot correspondant à un index donné.\n",
    "    \"\"\"\n",
    "    return jnp.where(\n",
    "        dim == 1,\n",
    "        index,\n",
    "        jnp.log2(index * (dim - 1) + 1) / jnp.log2(dim) + 1e-10\n",
    "    ).astype(int)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Index -> mot (reconstruction explicite)\n",
    "# -------------------------------------------------\n",
    "@jax.jit\n",
    "def index_to_word(index: int, dim: int) -> jnp.int64:\n",
    "    \"\"\"\n",
    "    Reconstruit le mot (entier) correspondant à un index.\n",
    "    \"\"\"\n",
    "    index = jnp.asarray(index, dtype=jnp.int64)\n",
    "    dim = jnp.asarray(dim, dtype=jnp.int64)\n",
    "\n",
    "    length = jnp.where(\n",
    "        dim == 1,\n",
    "        index,\n",
    "        jnp.floor(jnp.log2(index * (dim - 1) + 1) / jnp.log2(dim) + 1e-10)\n",
    "    ).astype(jnp.int64)\n",
    "\n",
    "    index = jnp.where(\n",
    "        dim == 1,\n",
    "        0,\n",
    "        index - (dim ** length - 1) // (dim - 1)\n",
    "    )\n",
    "\n",
    "    def body_fun(i, state):\n",
    "        word, remainder = state\n",
    "        power = dim ** (length - 1 - i)\n",
    "        digit = remainder // power\n",
    "        remainder = remainder % power\n",
    "        word = 10 * word + (digit + 1)\n",
    "        return word, remainder\n",
    "\n",
    "    word, _ = lax.fori_loop(\n",
    "        lower=0,\n",
    "        upper=length,\n",
    "        body_fun=body_fun,\n",
    "        init_val=(jnp.int64(0), index)\n",
    "    )\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Somme des lambdas associées à un mot (via index)\n",
    "# -------------------------------------------------\n",
    "@jax.jit\n",
    "def index_to_lam_sum(index: int, dim: int, lam: jax.Array) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Calcule la somme des coefficients lambda correspondant\n",
    "    aux lettres du mot associé à `index`.\n",
    "    \"\"\"\n",
    "    index = jnp.asarray(index, dtype=jnp.int64)\n",
    "    dim = jnp.asarray(dim, dtype=jnp.int64)\n",
    "\n",
    "    length = jnp.where(\n",
    "        dim == 1,\n",
    "        index,\n",
    "        jnp.floor(jnp.log2(index * (dim - 1) + 1) / jnp.log2(dim) + 1e-10)\n",
    "    ).astype(jnp.int64)\n",
    "\n",
    "    index = jnp.where(\n",
    "        dim == 1,\n",
    "        0,\n",
    "        index - (dim ** length - 1) // (dim - 1)\n",
    "    )\n",
    "\n",
    "    def body_fun(i, state):\n",
    "        acc, remainder = state\n",
    "        power = dim ** (length - 1 - i)\n",
    "        digit = remainder // power\n",
    "        remainder = remainder % power\n",
    "        acc = acc + lam[digit]\n",
    "        return acc, remainder\n",
    "\n",
    "    acc, _ = lax.fori_loop(\n",
    "        lower=0,\n",
    "        upper=length,\n",
    "        body_fun=body_fun,\n",
    "        init_val=(0.0, index)\n",
    "    )\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Mot -> nombre en base dim\n",
    "# -------------------------------------------------\n",
    "@jax.jit\n",
    "def word_to_base_dim_number(word: int, dim: int) -> int:\n",
    "    \"\"\"\n",
    "    Convertit un mot (ex: 231) en un nombre en base `dim`.\n",
    "    \"\"\"\n",
    "    def cond(state):\n",
    "        w, _, _ = state\n",
    "        return w > 0\n",
    "\n",
    "    def body(state):\n",
    "        w, acc, p = state\n",
    "        acc += ((w % 10) - 1) * (dim ** p)\n",
    "        w //= 10\n",
    "        p += 1\n",
    "        return w, acc, p\n",
    "\n",
    "    _, result, _ = lax.while_loop(cond, body, (word, 0, 0))\n",
    "    return result\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Mot -> index global\n",
    "# -------------------------------------------------\n",
    "@jax.jit\n",
    "def word_to_index(word: int, dim: int) -> int:\n",
    "    \"\"\"\n",
    "    Associe à un mot son index unique dans l’algèbre tensorielle.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        number_of_words_up_to_trunc(word_len(word) - 1, dim)\n",
    "        + word_to_base_dim_number(word, dim)\n",
    "    )\n",
    "\n",
    "\n",
    "# Vectorisations utiles\n",
    "word_to_base_dim_number_vect = jax.jit(jax.vmap(word_to_base_dim_number, in_axes=(0, None)))\n",
    "word_to_index_vect = jax.jit(jax.vmap(word_to_index, in_axes=(0, None)))\n",
    "index_to_word_vect = jax.jit(jax.vmap(index_to_word, in_axes=(0, None)))\n",
    "index_to_lam_sum_vect = jax.jit(jax.vmap(index_to_lam_sum, in_axes=(0, None, None)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988742f2-c0c9-4206-b9eb-76322602ace7",
   "metadata": {},
   "source": [
    "# 2️⃣ TensorSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320647d9-d1ac-43e8-8416-1dcd23858af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax_dataclasses as jdc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union, Tuple\n",
    "\n",
    "\n",
    "@jdc.pytree_dataclass\n",
    "\n",
    "class TensorSequence:\n",
    "    \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Représente un élément de l’algèbre tensorielle tronquée.\n",
    "\n",
    "    Mathématiquement :\n",
    "        T = somme_{|v| ≤ trunc} a_v · v\n",
    "\n",
    "    où les mots v sont codés implicitement par les indices du tableau `array`.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    array: jax.Array   # Tableau des coefficients (indexés par les mots)\n",
    "    trunc: int         # Longueur maximale des mots\n",
    "    dim: int           # Dimension de l’alphabet {1, …, dim}\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Représentation texte (pour affichage)\n",
    "    # ---------------------------------------------------------\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.array)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Affiche la TensorSequence comme une somme formelle :\n",
    "            1*Ø + 0.5*1 + 0.3*12 + ...\n",
    "        \"\"\"\n",
    "        res = \"\"\n",
    "        premier = True\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            coef = self.array[i].squeeze()\n",
    "\n",
    "            if not np.allclose(coef, 0):\n",
    "                if not premier:\n",
    "                    res += \" + \"\n",
    "                if np.isreal(coef):\n",
    "                    coef = coef.real\n",
    "                res += f\"{coef}*{index_to_word(i, self.dim)}\"\n",
    "                premier = False\n",
    "\n",
    "        return res if res else \"0\"\n",
    "\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # Taille et vérité logique\n",
    "    # ---------------------------------------------------------\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Nombre total de coefficients (incluant les zéros).\n",
    "        \"\"\"\n",
    "        return self.array.shape[0]\n",
    "\n",
    "    def __bool__(self) -> bool:\n",
    "        \"\"\"\n",
    "        Renvoie True si au moins un coefficient est non nul.\n",
    "        \"\"\"\n",
    "        return not np.allclose(self.array, 0)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Accès direct aux coefficients\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Accès direct au coefficient d’index `key`.\n",
    "        \"\"\"\n",
    "        return self.array[key]\n",
    "\n",
    "    def subsequence(self, key: Tuple):\n",
    "        \"\"\"\n",
    "        Extrait une sous-séquence (utile pour les trajectoires).\n",
    "        \"\"\"\n",
    "        return TensorSequence(\n",
    "            array=self.array[(slice(None), *key)],\n",
    "            trunc=self.trunc,\n",
    "            dim=self.dim\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Opérations algébriques de base\n",
    "    # ---------------------------------------------------------\n",
    "    def __rmul__(self, c: Union[float, complex, jax.Array]) -> TensorSequence:\n",
    "        \"\"\"\n",
    "        Multiplication scalaire à droite.\n",
    "        \"\"\"\n",
    "        return TensorSequence(self.array * c, self.trunc, self.dim)\n",
    "\n",
    "    def __mul__(self, c: Union[float, complex, jax.Array]) -> TensorSequence:\n",
    "        \"\"\"\n",
    "        Multiplication scalaire à gauche.\n",
    "        \"\"\"\n",
    "        return self.__rmul__(c)\n",
    "\n",
    "    def __truediv__(self, c: Union[float, complex, jax.Array]) -> TensorSequence:\n",
    "        \"\"\"\n",
    "        Division par un scalaire.\n",
    "        \"\"\"\n",
    "        return self * (1 / c)\n",
    "\n",
    "    def __add__(self, ts: TensorSequence) -> TensorSequence:\n",
    "        \"\"\"\n",
    "        Addition de deux TensorSequence.\n",
    "        \"\"\"\n",
    "        return TensorSequence(\n",
    "            self.array + ts.array,\n",
    "            trunc=jnp.maximum(self.trunc, ts.trunc),\n",
    "            dim=self.dim\n",
    "        )\n",
    "\n",
    "    def __sub__(self, ts: TensorSequence) -> TensorSequence:\n",
    "        \"\"\"\n",
    "        Soustraction de deux TensorSequence.\n",
    "        \"\"\"\n",
    "        return TensorSequence(\n",
    "            self.array - ts.array,\n",
    "            trunc=jnp.maximum(self.trunc, ts.trunc),\n",
    "            dim=self.dim\n",
    "        )\n",
    "\n",
    "    def __matmul__(self, ts: TensorSequence) -> Union[float, jax.Array]:\n",
    "        \"\"\"\n",
    "        Produit scalaire :\n",
    "            <T1, T2> = somme_v T1[v] * T2[v]\n",
    "        \"\"\"\n",
    "        return jnp.einsum(\"i..., i... -> ...\", self.array, ts.array)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Propriétés utiles\n",
    "    # ---------------------------------------------------------\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int, ...]:\n",
    "        \"\"\"\n",
    "        Forme du tableau de coefficients.\n",
    "        \"\"\"\n",
    "        return self.array.shape\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Projection sur un mot\n",
    "    # ---------------------------------------------------------\n",
    "    @jax.jit\n",
    "    def proj(self, word: int) -> TensorSequence:\n",
    "        \"\"\"\n",
    "        Projection de la TensorSequence sur un mot donné.\n",
    "\n",
    "        Mathématiquement :\n",
    "            Proj_v(T)(u) = T(vu)\n",
    "        \"\"\"\n",
    "        indices = jnp.arange(len(self))\n",
    "        longueur_mot = word_len(word)\n",
    "        index_mot = word_to_index(word, self.dim)\n",
    "\n",
    "        # Masque des indices valides\n",
    "        masque = (\n",
    "            ((indices - index_mot) % self.dim ** longueur_mot == 0)\n",
    "            & (indices >= index_mot)\n",
    "        )\n",
    "\n",
    "        longueurs = index_to_word_len(indices, self.dim)\n",
    "\n",
    "        nouveaux_indices = jnp.where(\n",
    "            self.dim == 1,\n",
    "            indices - index_mot,\n",
    "            (indices - self.dim ** longueurs + 1)\n",
    "            // self.dim ** longueur_mot\n",
    "            + self.dim ** (longueurs - longueur_mot) - 1\n",
    "        )\n",
    "\n",
    "        nouveaux_indices = jnp.where(masque, nouveaux_indices, len(self) + 1)\n",
    "\n",
    "        nouvel_array = jnp.zeros_like(self.array)\n",
    "        nouvel_array = nouvel_array.at[nouveaux_indices].set(\n",
    "            jnp.where(\n",
    "                jnp.einsum(\"i..., i -> i...\", jnp.ones_like(self.array), masque),\n",
    "                jnp.einsum(\"i..., i -> i...\", self.array, masque),\n",
    "                0\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return TensorSequence(nouvel_array, self.trunc, self.dim)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Outils internes\n",
    "    # ---------------------------------------------------------\n",
    "    @jax.jit\n",
    "    def get_lengths_array(self) -> jax.Array:\n",
    "        \"\"\"\n",
    "        Renvoie la longueur de chaque mot indexé.\n",
    "        \"\"\"\n",
    "        return index_to_word_len(jnp.arange(len(self)), self.dim)\n",
    "\n",
    "    @jax.jit\n",
    "    def get_lambdas_sum_array(self, lam: jax.Array) -> jax.Array:\n",
    "        \"\"\"\n",
    "        Renvoie la somme des lambda associée à chaque mot.\n",
    "        \"\"\"\n",
    "        return index_to_lam_sum_vect(jnp.arange(len(self)), self.dim, lam)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Visualisation\n",
    "    # ---------------------------------------------------------\n",
    "    def plot(self, trunc: int = None, ax: plt.axis = None, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Affiche les coefficients de la TensorSequence.\n",
    "        \"\"\"\n",
    "        if trunc is None:\n",
    "            trunc = self.trunc\n",
    "\n",
    "        n = number_of_words_up_to_trunc(trunc, self.dim)\n",
    "        indices = np.arange(n)\n",
    "        valeurs = np.zeros(n)\n",
    "        valeurs[:min(n, len(self))] = self.array[:min(n, len(self))]\n",
    "\n",
    "        if ax is None:\n",
    "            _, ax = plt.subplots()\n",
    "\n",
    "        ax.plot(valeurs, \"o\", **kwargs)\n",
    "        ax.set_xticks(indices)\n",
    "        ax.set_xticklabels(\n",
    "            [str(index_to_word(i, self.dim)) for i in indices],\n",
    "            rotation=-90\n",
    "        )\n",
    "        ax.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16997db-46a9-4825-ab58-db461c489061",
   "metadata": {},
   "source": [
    "# Factor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99e1d37-17d5-45b7-9808-0e62c8f3c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ZÉRO — élément nul de l’algèbre tensorielle\n",
    "# ---------------------------------------------------------\n",
    "def zero(trunc: int, dim: int) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Crée la TensorSequence nulle.\n",
    "\n",
    "    Mathématiquement :\n",
    "        0 = somme_v 0 · v\n",
    "\n",
    "    Tous les coefficients sont nuls.\n",
    "    \"\"\"\n",
    "    n = number_of_words_up_to_trunc(trunc=trunc, dim=dim)\n",
    "    array = jnp.zeros(n)\n",
    "    return TensorSequence(array=array, trunc=trunc, dim=dim)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# UNITÉ — élément neutre de l’algèbre tensorielle\n",
    "# ---------------------------------------------------------\n",
    "def unit(trunc: int, dim: int) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Crée l’élément unité de l’algèbre tensorielle.\n",
    "\n",
    "    Mathématiquement :\n",
    "        1 = 1·Ø\n",
    "\n",
    "    Le coefficient du mot vide (index 0) vaut 1,\n",
    "    tous les autres valent 0.\n",
    "    \"\"\"\n",
    "    n = number_of_words_up_to_trunc(trunc=trunc, dim=dim)\n",
    "    array = jnp.zeros(n)\n",
    "    array = array.at[0].set(1.0)\n",
    "    return TensorSequence(array=array, trunc=trunc, dim=dim)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ZÉRO \"LIKE\" — même forme qu’une TensorSequence existante\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def zero_like(ts: TensorSequence) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Crée une TensorSequence nulle ayant exactement la même forme\n",
    "    que `ts`.\n",
    "\n",
    "    Utile dans les boucles JAX (tensor_prod, shuffle, etc.).\n",
    "    \"\"\"\n",
    "    return TensorSequence(\n",
    "        array=jnp.zeros_like(ts.array),\n",
    "        trunc=ts.trunc,\n",
    "        dim=ts.dim\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# UNITÉ \"LIKE\" — même contexte qu’une TensorSequence existante\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def unit_like(ts: TensorSequence) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Crée l’élément unité dans le même espace tensoriel que `ts`.\n",
    "    \"\"\"\n",
    "    array = jnp.zeros_like(ts.array)\n",
    "    array = array.at[0].set(1.0)\n",
    "    return TensorSequence(array=array, trunc=ts.trunc, dim=ts.dim)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MOT CANONIQUE — un seul mot avec coefficient 1\n",
    "# ---------------------------------------------------------\n",
    "def from_word(word: int, trunc: int, dim: int) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Crée une TensorSequence correspondant à un mot unique.\n",
    "\n",
    "    Exemple :\n",
    "        from_word(12, trunc=3, dim=2)\n",
    "        → 1·12\n",
    "    \"\"\"\n",
    "    n = number_of_words_up_to_trunc(trunc=trunc, dim=dim)\n",
    "    array = jnp.zeros(n)\n",
    "\n",
    "    index = word_to_index(word=word, dim=dim)\n",
    "    array = array.at[index].set(1.0)\n",
    "\n",
    "    return TensorSequence(array=array, trunc=trunc, dim=dim)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# DEPUIS UN DICTIONNAIRE {mot: coefficient}\n",
    "# ---------------------------------------------------------\n",
    "def from_dict(word_dict: dict, trunc: int, dim: int) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Crée une TensorSequence à partir d’un dictionnaire :\n",
    "\n",
    "        { mot (int) : coefficient }\n",
    "\n",
    "    Exemple :\n",
    "        {12: 0.5, 21: -0.2}\n",
    "    \"\"\"\n",
    "    n = number_of_words_up_to_trunc(trunc=trunc, dim=dim)\n",
    "    array = jnp.zeros(n)\n",
    "\n",
    "    if not word_dict:\n",
    "        return TensorSequence(array=array, trunc=trunc, dim=dim)\n",
    "\n",
    "    for word, coef in word_dict.items():\n",
    "        index = word_to_index(word=word, dim=dim)\n",
    "        array = array.at[index].set(coef)\n",
    "\n",
    "    return TensorSequence(array=array, trunc=trunc, dim=dim)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# DEPUIS UN TABLEAU BRUT\n",
    "# ---------------------------------------------------------\n",
    "def from_array(array: jax.Array, trunc: int, dim: int) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Crée une TensorSequence à partir d’un tableau de coefficients.\n",
    "\n",
    "    ⚠️ Si le tableau est plus court que nécessaire,\n",
    "    il est automatiquement complété par des zéros.\n",
    "    \"\"\"\n",
    "    n = number_of_words_up_to_trunc(trunc=trunc, dim=dim)\n",
    "\n",
    "    array_ts = jnp.zeros((n,) + array.shape[1:], dtype=array.dtype)\n",
    "    array_ts = array_ts.at[:array.shape[0]].set(array)\n",
    "\n",
    "    return TensorSequence(array=array_ts, trunc=trunc, dim=dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615b448-e5a7-4ed0-a1e6-91ec7b174f4d",
   "metadata": {},
   "source": [
    "# Algebra_basis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "065b579d-066c-4f69-a96b-2a058c072a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "algebra_basis.py\n",
    "================\n",
    "\n",
    "Ce fichier définit une BASE CANONIQUE de l’algèbre tensorielle.\n",
    "\n",
    "L’idée :\n",
    "    - chaque mot correspond à un vecteur de base\n",
    "    - ces vecteurs sont créés à la demande (lazy)\n",
    "    - et mis en cache pour éviter les recalculs\n",
    "\n",
    "Usage typique :\n",
    "    basis = AlgebraBasis(dim=2, trunc=3)\n",
    "    e_12 = basis[12]   # TensorSequence correspondant au mot 12\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class AlgebraBasis:\n",
    "    \"\"\"\n",
    "    Base canonique paresseuse de l’algèbre tensorielle.\n",
    "\n",
    "    basis[word] = TensorSequence correspondant au mot `word`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, trunc: int):\n",
    "        \"\"\"\n",
    "        Initialise la base.\n",
    "\n",
    "        Paramètres\n",
    "        ----------\n",
    "        dim : int\n",
    "            Dimension de l’alphabet {1, …, dim}\n",
    "        trunc : int\n",
    "            Longueur maximale des mots\n",
    "        \"\"\"\n",
    "        self._cache = {}     # dictionnaire : word -> TensorSequence\n",
    "        self._dim = dim\n",
    "        self._trunc = trunc\n",
    "\n",
    "    def __getitem__(self, word: int):\n",
    "        \"\"\"\n",
    "        Accès à l’élément de base associé à `word`.\n",
    "\n",
    "        Si l’élément n’existe pas encore :\n",
    "            - il est créé avec from_word\n",
    "            - stocké dans le cache\n",
    "            - puis retourné\n",
    "        \"\"\"\n",
    "        if word not in self._cache:\n",
    "            self._cache[word] = from_word(\n",
    "                word=word,\n",
    "                trunc=self._trunc,\n",
    "                dim=self._dim\n",
    "            )\n",
    "        return self._cache[word]\n",
    "\n",
    "    @property\n",
    "    def trunc(self):\n",
    "        \"\"\"\n",
    "        Ordre de troncature de la base.\n",
    "        \"\"\"\n",
    "        return self._trunc\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        \"\"\"\n",
    "        Dimension de l’alphabet.\n",
    "        \"\"\"\n",
    "        return self._dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b60f4-0cca-4e9b-b834-f40ed0b38c2d",
   "metadata": {},
   "source": [
    "# Tensor_prod_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f625884-948e-44cb-9e58-30fbe01f4f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tensor_product.py\n",
    "=================\n",
    "\n",
    "Ce fichier implémente le PRODUIT TENSORIEL dans l’algèbre tensorielle\n",
    "des mots.\n",
    "\n",
    "Idée mathématique :\n",
    "    (u) ⊗ (v) = (uv)   ← concaténation des mots\n",
    "\n",
    "Si :\n",
    "    T1 = ∑ a_u · u\n",
    "    T2 = ∑ b_v · v\n",
    "\n",
    "Alors :\n",
    "    T1 ⊗ T2 = ∑ a_u b_v · (uv)\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Produit tensoriel avec un MOT\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def tensor_prod_word(ts: TensorSequence, word: int) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Produit tensoriel d’une TensorSequence avec un mot.\n",
    "\n",
    "    Mathématiquement :\n",
    "        T ⊗ w = ∑_u a_u · (u w)\n",
    "\n",
    "    où (u w) est la concaténation du mot u avec le mot w.\n",
    "    \"\"\"\n",
    "    indices = jnp.arange(len(ts))\n",
    "\n",
    "    # Longueur et position lexicographique du mot\n",
    "    longueur_mot = word_len(word)\n",
    "    mot_base = word_to_base_dim_number(word, dim=ts.dim)\n",
    "\n",
    "    # Longueur des mots correspondant aux indices\n",
    "    longueurs_indices = index_to_word_len(indices, dim=ts.dim)\n",
    "\n",
    "    # Position locale des mots (sans les mots plus courts)\n",
    "    indices_base = indices - number_of_words_up_to_trunc(\n",
    "        longueurs_indices - 1, ts.dim\n",
    "    )\n",
    "\n",
    "    # Calcul du nouvel index correspondant au mot concaténé\n",
    "    nouveaux_indices = (\n",
    "        number_of_words_up_to_trunc(\n",
    "            longueurs_indices + longueur_mot - 1, ts.dim\n",
    "        )\n",
    "        + ts.dim ** longueur_mot * indices_base\n",
    "        + mot_base\n",
    "    )\n",
    "\n",
    "    # Création du nouveau tableau de coefficients\n",
    "    nouvel_array = jnp.zeros_like(ts.array)\n",
    "    nouvel_array = nouvel_array.at[nouveaux_indices].set(ts.array)\n",
    "\n",
    "    return TensorSequence(\n",
    "        array=nouvel_array,\n",
    "        trunc=ts.trunc,\n",
    "        dim=ts.dim\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Produit tensoriel interne avec un index\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def _tensor_prod_index(\n",
    "    ts: TensorSequence,\n",
    "    index: int,\n",
    "    coefficient: Union[float, jax.Array] = 1.0\n",
    ") -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Produit tensoriel interne entre une TensorSequence et\n",
    "    un mot représenté par son index.\n",
    "\n",
    "    Utilisé dans tensor_prod(ts1, ts2).\n",
    "    \"\"\"\n",
    "    indices = jnp.arange(len(ts))\n",
    "    dim = ts.dim\n",
    "\n",
    "    # Longueur du mot associé à l’index\n",
    "    longueur_autre = index_to_word_len(jnp.array([index]), dim=dim)\n",
    "\n",
    "    # Position locale du mot\n",
    "    base_autre = index - number_of_words_up_to_trunc(longueur_autre - 1, dim)\n",
    "\n",
    "    # Informations sur les mots de ts\n",
    "    longueurs_indices = index_to_word_len(indices, dim=dim)\n",
    "    indices_base = indices - number_of_words_up_to_trunc(\n",
    "        longueurs_indices - 1, dim\n",
    "    )\n",
    "\n",
    "    # Nouveaux indices après concaténation\n",
    "    nouveaux_indices = (\n",
    "        number_of_words_up_to_trunc(\n",
    "            longueurs_indices + longueur_autre - 1, dim\n",
    "        )\n",
    "        + dim ** longueur_autre * indices_base\n",
    "        + base_autre\n",
    "    )\n",
    "\n",
    "    nouvel_array = jnp.zeros_like(ts.array)\n",
    "    nouvel_array = nouvel_array.at[nouveaux_indices].set(\n",
    "        ts.array * coefficient\n",
    "    )\n",
    "\n",
    "    return TensorSequence(\n",
    "        array=nouvel_array,\n",
    "        trunc=ts.trunc,\n",
    "        dim=dim\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Produit tensoriel entre DEUX TensorSequence\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def tensor_prod(ts1: TensorSequence, ts2: TensorSequence) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Produit tensoriel de deux TensorSequence.\n",
    "\n",
    "    Mathématiquement :\n",
    "        (∑ a_u u) ⊗ (∑ b_v v) = ∑ a_u b_v (u v)\n",
    "    \"\"\"\n",
    "\n",
    "    array2 = ts2.array\n",
    "\n",
    "    def corps(i, acc):\n",
    "        coef = array2[i]\n",
    "        return jax.lax.cond(\n",
    "            jnp.allclose(coef, 0),\n",
    "            lambda: acc,\n",
    "            lambda: acc + _tensor_prod_index(ts1, i, coef)\n",
    "        )\n",
    "\n",
    "    resultat = jax.lax.fori_loop(\n",
    "        lower=0,\n",
    "        upper=len(ts2),\n",
    "        body_fun=corps,\n",
    "        init_val=zero_like(ts1)\n",
    "    )\n",
    "\n",
    "    return resultat\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Puissance tensorielle\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def tensor_pow(ts: TensorSequence, p: int) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Calcule la puissance tensorielle :\n",
    "\n",
    "        ts ⊗ ts ⊗ ... ⊗ ts   (p fois)\n",
    "    \"\"\"\n",
    "    def corps(i, acc):\n",
    "        return tensor_prod(acc, ts)\n",
    "\n",
    "    return jax.lax.fori_loop(\n",
    "        lower=0,\n",
    "        upper=p,\n",
    "        body_fun=corps,\n",
    "        init_val=unit_like(ts)\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Exponentielle tensorielle\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def tensor_exp(ts: TensorSequence) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Calcule l’exponentielle tensorielle :\n",
    "\n",
    "        exp(ts) = ∑ (ts^⊗n) / n!\n",
    "\n",
    "    utilisée dans la définition des signatures.\n",
    "    \"\"\"\n",
    "    x = ts - ts[0] * unit_like(ts)\n",
    "\n",
    "    def corps(n, carry):\n",
    "        ts_puissance, factorielle, somme = carry\n",
    "        ts_puissance = tensor_prod(ts_puissance, x)\n",
    "        factorielle = factorielle * n\n",
    "        somme = somme + ts_puissance / factorielle\n",
    "        return ts_puissance, factorielle, somme\n",
    "\n",
    "    init = (unit_like(ts), 1.0, unit_like(ts))\n",
    "    _, _, resultat = jax.lax.fori_loop(\n",
    "        lower=1,\n",
    "        upper=ts.trunc + 1,\n",
    "        body_fun=corps,\n",
    "        init_val=init\n",
    "    )\n",
    "\n",
    "    return resultat * jnp.exp(ts[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e0be2-ecfa-46c7-bdf5-dbe95f2e310a",
   "metadata": {},
   "source": [
    "# shuffle_table.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36b1d0b-3fe4-466c-9232-956c70ad29c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "shuffle_table.py\n",
    "================\n",
    "\n",
    "Ce fichier construit la TABLE DE SHUFFLE.\n",
    "\n",
    "La table de shuffle encode le produit de Chen (produit shuffle)\n",
    "entre deux mots de l’algèbre tensorielle.\n",
    "\n",
    "Idée mathématique :\n",
    "    u ⧢ v = somme de tous les entrelacements possibles\n",
    "            des lettres de u et v, en respectant l’ordre interne.\n",
    "\n",
    "Exemple :\n",
    "    1 ⧢ 2 = 12 + 21\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import numba as nb\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Construction de la table de shuffle\n",
    "# ---------------------------------------------------------\n",
    "def get_shuffle_table(table_trunc: int, dim: int):\n",
    "    \"\"\"\n",
    "    Construit la table de shuffle pour tous les mots\n",
    "    de longueur ≤ table_trunc.\n",
    "\n",
    "    La table est utilisée pour accélérer le produit shuffle\n",
    "    entre deux TensorSequence.\n",
    "\n",
    "    Retour :\n",
    "        tableau numpy de forme (4, N) contenant :\n",
    "        - index gauche\n",
    "        - index droit\n",
    "        - index résultat\n",
    "        - multiplicité (nombre de shuffles identiques)\n",
    "    \"\"\"\n",
    "    # Nombre total de mots\n",
    "    n_mots = number_of_words_up_to_trunc(table_trunc, dim)\n",
    "\n",
    "    blocs = []\n",
    "    taille_totale = 0\n",
    "\n",
    "    # Toutes les paires d’indices possibles (i, j)\n",
    "    ij = np.array(\n",
    "        np.meshgrid(np.arange(n_mots), np.arange(n_mots))\n",
    "    ).T.reshape(-1, 2)\n",
    "\n",
    "    # On garde uniquement les paires dont la longueur totale\n",
    "    # ne dépasse pas la troncature\n",
    "    longueurs = index_to_word_len(jnp.array(ij), dim=dim).sum(axis=1)\n",
    "    ij = ij[longueurs <= table_trunc]\n",
    "\n",
    "    # Pour chaque paire de mots\n",
    "    for i, j in ij:\n",
    "        mot_i = int(index_to_word(i, dim))\n",
    "        mot_j = int(index_to_word(j, dim))\n",
    "\n",
    "        # Calcul du shuffle au niveau des MOTS\n",
    "        mots_res, comptes = shuffle_product_words(mot_i, mot_j)\n",
    "\n",
    "        # Conversion des mots résultats en indices\n",
    "        indices_res = word_to_index_vect(\n",
    "            jnp.array(mots_res, dtype=jnp.int64),\n",
    "            dim\n",
    "        )\n",
    "\n",
    "        # Bloc : [i, j, index_resultat, multiplicité]\n",
    "        bloc = np.zeros((len(comptes), 4), dtype=int)\n",
    "        bloc[:, 0] = i\n",
    "        bloc[:, 1] = j\n",
    "        bloc[:, 2] = indices_res\n",
    "        bloc[:, 3] = comptes\n",
    "\n",
    "        taille_totale += bloc.shape[0]\n",
    "        blocs.append(bloc)\n",
    "\n",
    "    # Concaténation finale\n",
    "    table_shuffle = np.vstack(blocs)\n",
    "\n",
    "    # Format attendu : (4, N)\n",
    "    return table_shuffle.T\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Shuffle de deux MOTS (niveau symbolique)\n",
    "# ---------------------------------------------------------\n",
    "@nb.jit(nopython=True)\n",
    "def shuffle_product_words(word_1: int, word_2: int) -> Tuple:\n",
    "    \"\"\"\n",
    "    Calcule le shuffle de deux mots (entiers).\n",
    "\n",
    "    Retour :\n",
    "        - tableau des mots résultants\n",
    "        - tableau des multiplicités associées\n",
    "    \"\"\"\n",
    "    # Cas du mot vide\n",
    "    if word_1 == 0:\n",
    "        return np.array([word_2], dtype=np.int64), np.ones(1, dtype=np.int64)\n",
    "    if word_2 == 0:\n",
    "        return np.array([word_1], dtype=np.int64), np.ones(1, dtype=np.int64)\n",
    "\n",
    "    # Longueur des mots\n",
    "    l1 = int(np.log10(word_1)) + 1\n",
    "    l2 = int(np.log10(word_2)) + 1\n",
    "\n",
    "    # Concaténation brute\n",
    "    mot_concat = word_1 * 10**l2 + word_2\n",
    "\n",
    "    # Extraction des lettres\n",
    "    lettres = np.array([\n",
    "        mot_concat // 10**k % 10\n",
    "        for k in range(l1 + l2 - 1, -1, -1)\n",
    "    ])\n",
    "\n",
    "    # Indices de positions possibles\n",
    "    indices_gauche = combinations(np.arange(l1 + l2), l1)\n",
    "    indices_droite = combinations(np.arange(l1 + l2), l2)[::-1]\n",
    "\n",
    "    # Construction des shuffles\n",
    "    indices = np.zeros((indices_gauche.shape[0], l1 + l2), dtype=np.int64)\n",
    "    indices[:, :l1] = indices_gauche\n",
    "    indices[:, l1:] = indices_droite\n",
    "\n",
    "    puissances = 10**(l1 + l2 - 1 - indices)\n",
    "    shuffles = np.sum(puissances * lettres, axis=1)\n",
    "\n",
    "    # Comptage des duplications\n",
    "    shuffles_tries = np.sort(shuffles)\n",
    "    shuffles_tries = np.append(shuffles_tries, -1)\n",
    "\n",
    "    changements = np.where(np.diff(shuffles_tries) != 0)[0]\n",
    "    comptes = np.zeros(changements.size, dtype=np.int64)\n",
    "    comptes[0] = changements[0] + 1\n",
    "    comptes[1:] = np.diff(changements)\n",
    "\n",
    "    return shuffles_tries[changements], comptes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36497d6-ff45-4e69-b223-0cfdf4c90750",
   "metadata": {},
   "source": [
    "# shuffle_product.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3fada85-020c-48b8-9abc-b2f8c1577578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "shuffle_product.py\n",
    "==================\n",
    "\n",
    "Ce fichier implémente le PRODUIT SHUFFLE (ou produit de Chen)\n",
    "entre deux TensorSequence.\n",
    "\n",
    "Idée mathématique :\n",
    "------------------\n",
    "Si\n",
    "    T1 = ∑ a_u · u\n",
    "    T2 = ∑ b_v · v\n",
    "\n",
    "alors\n",
    "    T1 ⧢ T2 = ∑ a_u b_v · (u ⧢ v)\n",
    "\n",
    "où (u ⧢ v) est la somme de tous les entrelacements possibles\n",
    "des lettres de u et v, en respectant l’ordre interne de chaque mot.\n",
    "\n",
    "⚠️ Le calcul combinatoire est coûteux :\n",
    "→ on utilise une TABLE DE SHUFFLE pré-calculée (shuffle_table).\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Calcul du shuffle au niveau des TABLEAUX\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def _shuffle_prod_array(\n",
    "    array1: jax.Array,\n",
    "    array2: jax.Array,\n",
    "    shuffle_table: jax.Array,\n",
    "):\n",
    "    \"\"\"\n",
    "    Applique le produit shuffle au niveau des tableaux de coefficients.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    array1, array2 : jax.Array\n",
    "        Tableaux de coefficients des deux TensorSequence.\n",
    "    shuffle_table : jax.Array\n",
    "        Table de shuffle (4, N) :\n",
    "            - index gauche\n",
    "            - index droit\n",
    "            - index résultat\n",
    "            - multiplicité\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    jax.Array\n",
    "        Tableau de coefficients du produit shuffle.\n",
    "    \"\"\"\n",
    "    index_gauche, index_droit, index_resultat, multiplicite = shuffle_table\n",
    "\n",
    "    # Contribution de chaque shuffle :\n",
    "    # multiplicite × coeff_gauche × coeff_droit\n",
    "    contributions = (\n",
    "        multiplicite\n",
    "        * array1[index_gauche]\n",
    "        * array2[index_droit]\n",
    "    )\n",
    "\n",
    "    # Accumulation dans le tableau résultat\n",
    "    resultat = jnp.zeros_like(array1)\n",
    "    resultat = resultat.at[index_resultat].add(contributions)\n",
    "\n",
    "    return resultat\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Version vectorisée (pour tableaux 2D ou trajectoires)\n",
    "# ---------------------------------------------------------\n",
    "_shuffle_prod_array_vect = jax.jit(\n",
    "    jax.vmap(\n",
    "        _shuffle_prod_array,\n",
    "        in_axes=(1, 1, None),\n",
    "        out_axes=1\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Produit shuffle entre DEUX TensorSequence\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def shuffle_prod(\n",
    "    ts1: TensorSequence,\n",
    "    ts2: TensorSequence,\n",
    "    shuffle_table: jax.Array\n",
    ") -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Calcule le produit shuffle de deux TensorSequence.\n",
    "\n",
    "    Mathématiquement :\n",
    "        ts1 ⧢ ts2 = ∑ a_u b_v (u ⧢ v)\n",
    "    \"\"\"\n",
    "    array = _shuffle_prod_array(\n",
    "        ts1.array,\n",
    "        ts2.array,\n",
    "        shuffle_table\n",
    "    )\n",
    "\n",
    "    return TensorSequence(\n",
    "        array=array,\n",
    "        trunc=ts1.trunc,\n",
    "        dim=ts1.dim\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Produit shuffle pour TensorSequence 2D (trajectoires)\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def shuffle_prod_2d(\n",
    "    ts1: TensorSequence,\n",
    "    ts2: TensorSequence,\n",
    "    shuffle_table: jax.Array\n",
    ") -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Version du produit shuffle pour des TensorSequence\n",
    "    dont les coefficients dépendent du temps ou d’une autre dimension.\n",
    "\n",
    "    Typiquement :\n",
    "        ts.array.shape = (n_mots, n_temps)\n",
    "    \"\"\"\n",
    "    array = _shuffle_prod_array_vect(\n",
    "        ts1.array.reshape((len(ts1), -1)),\n",
    "        ts2.array.reshape((len(ts2), -1)),\n",
    "        shuffle_table\n",
    "    ).reshape(ts1.array.shape)\n",
    "\n",
    "    return TensorSequence(\n",
    "        array=array,\n",
    "        trunc=ts1.trunc,\n",
    "        dim=ts1.dim\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Puissance shuffle\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def shuffle_pow(\n",
    "    ts: TensorSequence,\n",
    "    p: int,\n",
    "    shuffle_table: jax.Array\n",
    ") -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Calcule la puissance shuffle :\n",
    "\n",
    "        ts^{⧢ p} = ts ⧢ ts ⧢ ... ⧢ ts  (p fois)\n",
    "    \"\"\"\n",
    "    def corps(i, acc):\n",
    "        return shuffle_prod(acc, ts, shuffle_table)\n",
    "\n",
    "    return jax.lax.fori_loop(\n",
    "        lower=0,\n",
    "        upper=p,\n",
    "        body_fun=corps,\n",
    "        init_val=unit_like(ts)\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Exponentielle shuffle\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def shuffle_exp(\n",
    "    ts: TensorSequence,\n",
    "    shuffle_table: jax.Array\n",
    ") -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Calcule l’exponentielle shuffle :\n",
    "\n",
    "        exp⧢(ts) = ∑ ts^{⧢n} / n!\n",
    "\n",
    "    Utilisée dans la théorie des signatures\n",
    "    (équivalent du développement de Chen).\n",
    "    \"\"\"\n",
    "    # On enlève la composante du mot vide\n",
    "    x = ts - ts[0] * unit_like(ts)\n",
    "\n",
    "    def corps(n, carry):\n",
    "        ts_puissance, factorielle, somme = carry\n",
    "        ts_puissance = shuffle_prod(ts_puissance, x, shuffle_table)\n",
    "        factorielle = factorielle * n\n",
    "        somme = somme + ts_puissance / factorielle\n",
    "        return ts_puissance, factorielle, somme\n",
    "\n",
    "    init = (unit_like(ts), 1.0, unit_like(ts))\n",
    "\n",
    "    _, _, resultat = jax.lax.fori_loop(\n",
    "        lower=1,\n",
    "        upper=ts.trunc + 1,\n",
    "        body_fun=corps,\n",
    "        init_val=init\n",
    "    )\n",
    "\n",
    "    # facteur exp(coefficient du mot vide)\n",
    "    return resultat * jnp.exp(ts[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f50b0f-a6ad-45ef-8a0b-ed79176676bb",
   "metadata": {},
   "source": [
    "# Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcdbc00f-873a-459b-8c33-26aff6bd3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "operators.py\n",
    "============\n",
    "\n",
    "Ce fichier définit les OPÉRATEURS LINÉAIRES fondamentaux\n",
    "agissant sur les TensorSequence.\n",
    "\n",
    "Ces opérateurs sont utilisés pour :\n",
    "- signatures à mémoire (FM-signatures)\n",
    "- équations différentielles sur les signatures\n",
    "- régularisation et résolvantes\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Opérateur G\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def G(ts: TensorSequence, lam: jax.Array) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Opérateur G : multiplication par la somme des λ du mot.\n",
    "\n",
    "    Pour chaque mot v :\n",
    "        (G ts)[v] = λ(v) · ts[v]\n",
    "\n",
    "    où :\n",
    "        λ(v) = somme des λ_i pour les lettres du mot v\n",
    "    \"\"\"\n",
    "    lams = ts.get_lambdas_sum_array(lam)\n",
    "    shape = (-1,) + (1,) * (ts.array.ndim - 1)\n",
    "\n",
    "    return TensorSequence(\n",
    "        array=ts.array * lams.reshape(shape),\n",
    "        trunc=ts.trunc,\n",
    "        dim=ts.dim\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Opérateur G^{-1} (pseudo-inverse)\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def G_inv(ts: TensorSequence, lam: jax.Array) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Pseudo-inverse de G.\n",
    "\n",
    "    Pour chaque mot v :\n",
    "        (G^{-1} ts)[v] = ts[v] / λ(v)   si λ(v) ≠ 0\n",
    "                         0            sinon\n",
    "\n",
    "    Le mot vide est toujours envoyé sur 0.\n",
    "    \"\"\"\n",
    "    lams = ts.get_lambdas_sum_array(lam)\n",
    "    shape = (-1,) + (1,) * (ts.array.ndim - 1)\n",
    "\n",
    "    coeffs = jnp.where(lams != 0, 1.0 / lams, 0.0)\n",
    "\n",
    "    return TensorSequence(\n",
    "        array=ts.array * coeffs.reshape(shape),\n",
    "        trunc=ts.trunc,\n",
    "        dim=ts.dim\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Opérateur de décote D\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def D(ts: TensorSequence, dt: float, lam: jax.Array) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Opérateur de décote exponentielle.\n",
    "\n",
    "    Pour chaque mot v :\n",
    "        (D ts)[v] = exp(-λ(v) · dt) · ts[v]\n",
    "\n",
    "    Utilisé pour les signatures à mémoire finie.\n",
    "    \"\"\"\n",
    "    lams = ts.get_lambdas_sum_array(lam)\n",
    "    shape = (-1,) + (1,) * (ts.array.ndim - 1)\n",
    "\n",
    "    facteur = jnp.exp(-lams * dt)\n",
    "\n",
    "    return TensorSequence(\n",
    "        array=ts.array * facteur.reshape(shape),\n",
    "        trunc=ts.trunc,\n",
    "        dim=ts.dim\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Décote dépendante du temps (vecteur dt)\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def D_timedep(ts: TensorSequence, dt: jax.Array, lam: jax.Array) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Version vectorisée de l’opérateur D pour un vecteur de temps.\n",
    "\n",
    "    Utilisée pour des trajectoires de signatures.\n",
    "    \"\"\"\n",
    "    lams = ts.get_lambdas_sum_array(lam)\n",
    "\n",
    "    shape = (\n",
    "        lams.size,\n",
    "        dt.size\n",
    "    ) + (1,) * (ts.array.ndim - 2)\n",
    "\n",
    "    facteur = jnp.exp(-jnp.outer(lams, dt))\n",
    "\n",
    "    return TensorSequence(\n",
    "        array=ts.array * facteur.reshape(shape),\n",
    "        trunc=ts.trunc,\n",
    "        dim=ts.dim\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Résolvante de G : (Id + G)^{-1}\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def G_resolvent(ts: TensorSequence, lam: jax.Array) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Opérateur résolvante :\n",
    "\n",
    "        (Id + G)^{-1}\n",
    "\n",
    "    Pour chaque mot v :\n",
    "        ts[v] / (1 + λ(v))\n",
    "    \"\"\"\n",
    "    lams = ts.get_lambdas_sum_array(lam)\n",
    "    shape = (-1,) + (1,) * (ts.array.ndim - 1)\n",
    "\n",
    "    return TensorSequence(\n",
    "        array=ts.array * (1.0 / (1.0 + lams)).reshape(shape),\n",
    "        trunc=ts.trunc,\n",
    "        dim=ts.dim\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Schéma semi-intégré\n",
    "# ---------------------------------------------------------\n",
    "@jax.jit\n",
    "def semi_integrated_scheme(\n",
    "    ts: TensorSequence,\n",
    "    dt: float,\n",
    "    lam: jax.Array\n",
    ") -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Schéma numérique pour l’équation :\n",
    "\n",
    "        ψ' = -G(ψ) + F\n",
    "\n",
    "    Implémente :\n",
    "        G^{-1}(Id - D)\n",
    "    \"\"\"\n",
    "    lams = ts.get_lambdas_sum_array(lam)\n",
    "    shape = (-1,) + (1,) * (ts.array.ndim - 1)\n",
    "\n",
    "    coeffs = jnp.where(\n",
    "        lams != 0,\n",
    "        (1.0 - jnp.exp(-lams * dt)) / lams,\n",
    "        dt\n",
    "    )\n",
    "\n",
    "    return TensorSequence(\n",
    "        array=ts.array * coeffs.reshape(shape),\n",
    "        trunc=ts.trunc,\n",
    "        dim=ts.dim\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Opérateur de décote exponentielle pour signatures à mémoire (EFM).\n",
    "\n",
    "@jax.jit\n",
    "def Phi(ts: TensorSequence,  dt_alpha: float, lam: jax.Array) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Opérateur de décote exponentielle pour signatures à mémoire (EFM).\n",
    "\n",
    "    Pour chaque mot v :\n",
    "        (Φ ts)[v] = exp( - λ(v) * Δα ) * ts[v]\n",
    "\n",
    "    où :\n",
    "        Δα = t^α - s^α\n",
    "        λ(v) = somme des λ_i correspondant aux lettres du mot v\n",
    "    \"\"\"\n",
    "    # λ(v) pour tous les mots\n",
    "    lams = ts.get_lambdas_sum_array(lam)\n",
    "\n",
    "    # reshape pour broadcast\n",
    "    shape = (-1,) + (1,) * (ts.array.ndim - 1)\n",
    "\n",
    "    facteur = jnp.exp(-lams * dt_alpha)\n",
    "\n",
    "    return TensorSequence(\n",
    "        array=ts.array * facteur.reshape(shape),\n",
    "        trunc=ts.trunc,\n",
    "        dim=ts.dim\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Générateur mémoire G_α pour les EFM-signatures\n",
    "\n",
    "@jax.jit\n",
    "def G_alpha(ts: TensorSequence, t: float, alpha: float, lam: jax.Array) -> TensorSequence:\n",
    "    \"\"\"\n",
    "    Générateur mémoire G_α pour les EFM-signatures.\n",
    "\n",
    "    Pour chaque mot v :\n",
    "        (G_α ts)[v] = α · t^{α-1} · λ(v) · ts[v]\n",
    "\n",
    "    où :\n",
    "        λ(v) = somme des λ_i associées aux lettres du mot v\n",
    "    \"\"\"\n",
    "    lams = ts.get_lambdas_sum_array(lam)\n",
    "\n",
    "    # broadcast\n",
    "    shape = (-1,) + (1,) * (ts.array.ndim - 1)\n",
    "\n",
    "    coef = alpha * (t ** (alpha - 1)) * lams\n",
    "\n",
    "    return TensorSequence(\n",
    "        array=ts.array * coef.reshape(shape),\n",
    "        trunc=ts.trunc,\n",
    "        dim=ts.dim\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ed861-006c-4f92-9262-2576c8d35b2c",
   "metadata": {},
   "source": [
    "# Path_signature.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8e8904-6d6a-49b3-a5cb-f25061469ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from math import factorial\n",
    "from scipy.special import hyp1f1  # 1F1 de Kummer\n",
    "\n",
    "# =========================================================\n",
    "# 0) Outils : taille signature\n",
    "# =========================================================\n",
    "def n_words_upto(trunc: int, dim: int) -> int:\n",
    "    # 1 + d + d^2 + ... + d^trunc\n",
    "    if dim == 1:\n",
    "        return trunc + 1\n",
    "    return (dim**(trunc + 1) - 1) // (dim - 1)\n",
    "\n",
    "# =========================================================\n",
    "# 1) Coefficient exact EFM : C(a,b)\n",
    "# =========================================================\n",
    "def C_efm(a: float, b: float, lam: float, alpha: float) -> float:\n",
    "    \"\"\"\n",
    "    C(a,b) = exp(-lam*b^alpha) * ( b*1F1(1/a,1+1/a,lam*b^a) - a*1F1(1/a,1+1/a,lam*a^a) ) / (b-a)\n",
    "    \"\"\"\n",
    "    if b <= a:\n",
    "        return 0.0\n",
    "\n",
    "    A = 1.0 / alpha\n",
    "    B = 1.0 + 1.0 / alpha\n",
    "\n",
    "    term_b = b * hyp1f1(A, B, lam * (b**alpha))\n",
    "    term_a = a * hyp1f1(A, B, lam * (a**alpha))\n",
    "\n",
    "    val = np.exp(-lam * (b**alpha)) * (term_b - term_a) / (b - a)\n",
    "\n",
    "    # sécurité : parfois hyp1f1 renvoie complex (numériquement), on prend la partie réelle\n",
    "    return float(np.real(val))\n",
    "\n",
    "# \n",
    "# 2) Objet signature : stockage niveau par niveau\n",
    "# \n",
    "@dataclass\n",
    "class SigLevels:\n",
    "    levels: list  # levels[n] shape (dim**n,)\n",
    "    dim: int\n",
    "    trunc: int\n",
    "\n",
    "    @staticmethod\n",
    "    def unit(trunc: int, dim: int) -> \"SigLevels\":\n",
    "        levels = [np.array([1.0])]\n",
    "        for n in range(1, trunc + 1):\n",
    "            levels.append(np.zeros(dim**n, dtype=float))\n",
    "        return SigLevels(levels=levels, dim=dim, trunc=trunc)\n",
    "\n",
    "    def copy(self) -> \"SigLevels\":\n",
    "        return SigLevels(levels=[lvl.copy() for lvl in self.levels], dim=self.dim, trunc=self.trunc)\n",
    "\n",
    "    def to_flat(self) -> np.ndarray:\n",
    "        return np.concatenate(self.levels, axis=0)\n",
    "\n",
    "# \n",
    "# 3) exp^{⊗}(w) quand w est un vecteur niveau 1\n",
    "# \n",
    "def tensor_exp_level1(w: np.ndarray, trunc: int) -> list:\n",
    "    \"\"\"\n",
    "    exp^{⊗}(w) :\n",
    "      level0 = 1\n",
    "      leveln = w^{⊗n}/n!\n",
    "    \"\"\"\n",
    "    w = np.asarray(w, float)\n",
    "    dim = w.size\n",
    "\n",
    "    levels = [np.array([1.0])]\n",
    "    if trunc == 0:\n",
    "        return levels\n",
    "\n",
    "    levels.append(w.copy())  # niveau 1\n",
    "    tp = w.copy()\n",
    "    for n in range(2, trunc + 1):\n",
    "        tp = np.kron(tp, w)           # w^{⊗n}\n",
    "        levels.append(tp / factorial(n))\n",
    "    return levels\n",
    "\n",
    "# \n",
    "# 4) Produit tensoriel tronqué (Chen)\n",
    "# \n",
    "def tensor_prod(a: SigLevels, b: SigLevels) -> SigLevels:\n",
    "    \"\"\"\n",
    "    (a ⊗ b)_n = sum_{k=0..n} a_k ⊗ b_{n-k}\n",
    "    \"\"\"\n",
    "    assert a.dim == b.dim and a.trunc == b.trunc\n",
    "    dim, trunc = a.dim, a.trunc\n",
    "    out = SigLevels.unit(trunc, dim)\n",
    "\n",
    "    for n in range(0, trunc + 1):\n",
    "        acc = np.zeros(dim**n, dtype=float)\n",
    "        for k in range(0, n + 1):\n",
    "            acc += np.kron(a.levels[k], b.levels[n - k])\n",
    "        out.levels[n] = acc\n",
    "    return out\n",
    "\n",
    "# \n",
    "# 5) Phi EFM (décote) — cas \"lam constant par lettre\"\n",
    "# \n",
    "def Phi_efm(sig: SigLevels, dt_alpha: float, lam_scalar: float) -> SigLevels:\n",
    "    \"\"\"\n",
    "    (Phi sig)^v = exp(-lam(v) * dt_alpha) sig^v\n",
    "\n",
    "    Ici on suppose lam constant par lettre (ou lam identique sur les dimensions),\n",
    "    donc lam(v) = |v| * lam_scalar.\n",
    "    \"\"\"\n",
    "    out = sig.copy()\n",
    "    for n in range(sig.trunc + 1):\n",
    "        out.levels[n] *= np.exp(-(n * lam_scalar) * dt_alpha)\n",
    "    return out\n",
    "\n",
    "# =========================================================\n",
    "# 6) EFM signature cumulée d'une trajectoire\n",
    "# =========================================================\n",
    "def efm_signature_trajectory(\n",
    "    x: np.ndarray,\n",
    "    t: np.ndarray,\n",
    "    trunc: int,\n",
    "    lam,\n",
    "    alpha: float,\n",
    "    return_traj: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    x : (N, dim)\n",
    "    t : (N,)\n",
    "    lam : scalaire OU vecteur (dim,) mais supposé identique sur toutes les dims pour C_efm\n",
    "    return_traj : si True -> renvoie (Sig_final, Sig_traj_flat)\n",
    "                  Sig_traj_flat shape = (n_features, N)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, float)\n",
    "    t = np.asarray(t, float)\n",
    "\n",
    "    if x.ndim != 2:\n",
    "        raise ValueError(\"x doit être de forme (N, dim).\")\n",
    "    if t.ndim != 1 or t.size != x.shape[0]:\n",
    "        raise ValueError(\"t doit être de forme (N,) et compatible avec x.\")\n",
    "    if np.any(np.diff(t) < 0):\n",
    "        raise ValueError(\"t doit être croissant.\")\n",
    "\n",
    "    N, dim = x.shape\n",
    "    lam = np.asarray(lam, float).reshape(-1)\n",
    "\n",
    "    if lam.size == 1:\n",
    "        lam_scalar = float(lam[0])\n",
    "    else:\n",
    "        # on accepte un vecteur seulement s'il est constant (même valeur partout)\n",
    "        if not np.allclose(lam, lam[0]):\n",
    "            raise ValueError(\"Cette version 'const_lam' requiert lam identique sur toutes les dimensions.\")\n",
    "        lam_scalar = float(lam[0])\n",
    "\n",
    "    # init\n",
    "    S = SigLevels.unit(trunc, dim)\n",
    "    n_feat = n_words_upto(trunc, dim)\n",
    "    traj = np.zeros((n_feat, N), dtype=float)\n",
    "    traj[:, 0] = S.to_flat()\n",
    "\n",
    "    # boucle segments\n",
    "    for k in range(1, N):\n",
    "        a, b = t[k - 1], t[k]\n",
    "        if b == a:\n",
    "            traj[:, k] = S.to_flat()\n",
    "            continue\n",
    "\n",
    "        dt_alpha = (b**alpha) - (a**alpha)\n",
    "\n",
    "        # 1) décote du passé\n",
    "        S = Phi_efm(S, dt_alpha, lam_scalar)\n",
    "\n",
    "        # 2) signature du segment (EFM)\n",
    "        dX = x[k] - x[k - 1]\n",
    "        v = dX \n",
    "        C = C_efm(a, b, lam_scalar, alpha)\n",
    "        w = C * v\n",
    "\n",
    "        seg_levels = tensor_exp_level1(w, trunc)\n",
    "        Sig_k = SigLevels(levels=seg_levels, dim=dim, trunc=trunc)\n",
    "\n",
    "        # 3) Chen\n",
    "        S = tensor_prod(S, Sig_k)\n",
    "\n",
    "        traj[:, k] = S.to_flat()\n",
    "\n",
    "    if return_traj:\n",
    "        return S, traj\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25791d3a-3989-4273-b731-fd735aa3613a",
   "metadata": {},
   "source": [
    "# Calcul alpha- EFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb6a09d0-2a37-4a3a-889a-9c4400e1e25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sig_traj shape = (15, 2001)\n",
      "Signature finale (10 premiers coeffs) = [ 1.          0.6603398  -1.257294    0.21802433 -0.78924402 -0.04099725\n",
      "  0.79039411  0.04799005 -0.26293732  0.0047054 ]\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 7) Exemple OU -> EFM signature ordre trunc\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Exemple : suppose que tu as déjà ton OU (t, x)\n",
    "    # Ici mini exemple:\n",
    "    t = np.linspace(0.0, 2.0, 2001)\n",
    "    x = np.zeros((t.size, 2))\n",
    "    x[:, 0] = np.sin(t)\n",
    "    x[:, 1] = np.cos(t)\n",
    "\n",
    "    trunc = 3\n",
    "    lam = 1.0\n",
    "    alpha = 0.2\n",
    "\n",
    "    Sig_final, Sig_traj = efm_signature_trajectory(x, t, trunc, lam, alpha, return_traj=True)\n",
    "\n",
    "    print(\"Sig_traj shape =\", Sig_traj.shape)  # (n_features, N)\n",
    "    print(\"Signature finale (10 premiers coeffs) =\", Sig_traj[:10, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19518d2c-e058-45f2-bf1e-99d06dbddc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64a7ef-0580-4ba1-946d-25ba0b0f5d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dd5807-fac9-43e5-9031-473b401d8afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f968c8-4691-4152-a86a-ff7b81a1b20e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af91da36-d034-4cc9-80b9-6afa20fbcd55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
